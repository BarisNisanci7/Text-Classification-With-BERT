{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Library Imports and Data Loading\n",
    "\n",
    "In this block, we import the necessary Python libraries (PyTorch, Transformers, scikit-learn, etc.). \n",
    "We also define `data_folder_path` and loop through each author directory to load the article files. \n",
    "Each article is stored as a tuple `(author, content)` in the `articles` list.\n"
   ],
   "id": "1f616a4172c51cc2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-29T12:31:26.695109Z",
     "start_time": "2025-01-29T12:31:20.411453Z"
    }
   },
   "source": [
    "# Step 1 - Library Imports and Data Loading\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# For creating DataLoader and TensorDataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Transformers: BERT tokenizer and classification model\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# scikit-learn tools: label encoding, k-fold, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the data folder path\n",
    "data_folder_path = \"data/finalDataset/makaleler-yazarlar\"\n",
    "\n",
    "# This list will hold (author, article_content) tuples\n",
    "articles = []\n",
    "\n",
    "# Loop through each author's folder and read article files\n",
    "for author_folder in os.listdir(data_folder_path):\n",
    "    author_path = os.path.join(data_folder_path, author_folder)\n",
    "    if os.path.isdir(author_path):\n",
    "        for article_file in os.listdir(author_path):\n",
    "            article_path = os.path.join(author_path, article_file)\n",
    "            \n",
    "            # Try-except block for handling various text encodings\n",
    "            try:\n",
    "                with open(article_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(article_path, 'r', encoding='ISO-8859-9') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "            articles.append((author_folder, content))\n",
    "\n",
    "print(f\"Total articles loaded: {len(articles)}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Deder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles loaded: 1500\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Text Preprocessing\n",
    "\n",
    "Here, we define a simple `preprocess_text` function. \n",
    "It replaces multiple whitespace characters with a single space, and strips leading/trailing spaces.\n",
    "You could optionally add more steps (stopword removal, punctuation handling, etc.) if desired.\n"
   ],
   "id": "eadfcafc61c00561"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T12:31:26.972192Z",
     "start_time": "2025-01-29T12:31:26.785089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2 - Text Preprocessing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    A simple preprocessing function:\n",
    "      - Replace multiple whitespaces with a single space\n",
    "      - Strip leading/trailing whitespace\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Convert multiple spaces to one\n",
    "    text = text.strip()               # Remove leading/trailing whitespace\n",
    "    \n",
    "    # If using a cased model for Turkish, you might keep original casing.\n",
    "    # You can modify or add steps as needed.\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to each article\n",
    "preprocessed_articles = [(author, preprocess_text(content)) for author, content in articles]\n",
    "\n",
    "# Print an example preprocessed article\n",
    "print(\"Example of a preprocessed article:\")\n",
    "print(preprocessed_articles[0])\n"
   ],
   "id": "e8335990c041a59c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a preprocessed article:\n",
      "('AHMET ÇAKAR', \"Fernando Muslera! Önce şunu belirtelim ki; Galatasaray yerli ve yabancı transferlerini mükemmel yapmış. Mesela kaleci Muslera... Kritik anlarda maçı kurtaran adam oldu. Mesela emektar Necati... Galatasaray'ın en faydalı oyuncusu. Gol attı; gol attırdı; ileride top tuttu; mükemmel oynadı. Üstelik attığı ilk gol de, yılın golleri arasına girebilecek güzellikte. 25 metreden ve kalecinin üzerinden öyle bir vurdu ki, maçın tüm stratejisini Galatasaray lehine değiştiriverdi. Skor kimseyi aldatmasın. Sivas, kendi çapında çok iyi bir takım. Üstelik saha ve hava şartları da onların alışkın olduğu cinstendi. Ve daha önemlisi, maçın hemen başında mağlup duruma düşmüş olsalar da, özellikle ilk yarıda disiplini hiç kaybetmediler. Ve çok da önemli pozisyonlar buldular. Fakat Galatasaray pozisyon verse de, önce kaleci Muslera, sonra da oyuncuların panik yapmaması sarı-kırmızılılarda dengeyi getirdi. Çünkü ilk yarıda yenecek bir gol, her şeyi berbat edebilirdi. İkinci yarı en kritik an Galatasaray'ın ikinci golü bulduğu an. Çünkü böylesine dişli rakiplere karşı deplasmanda ikinci golü bulduğunuzda tüm kozlar elinize geçiveriyor. Bu dakikadan sonra Sivas açısından yine kritik anlar var. Ama o anlarda sahneye yine Galatasaray'ın göbeğinde dengeli oynayan Ujfalusi ile Semih özellikle de yılın transferi Muslera çıktı. Bu dakikalardan sonra her kontratak Galatasaray için ya gol ya da gol pozisyonu getirdi. KADIKÖY'DE YENİLMEZSE... Türk futbol kamuoyu dün geceki maçı Galatasaray'ın kazanamayacağını düşünüyordu ama rakipler için evdeki hesap çarşıya uymadı. Eğer aynı Galatasaray iki hafta sonra Kadıköy'de kaybetmezse şampiyonluk için play-off'a dev bir adımla ilerlemiş olur. Gelelim hakem Halis Özkahya'ya... Kartlarını çok yerinde kullandı. Özellikle hava toplarında Melo ve Riera'ya verdiği sarı kartlar doğru. İkinci yarıda Eboue'ye yapılan harekette birçok kişi kırmızı kart beklerken hakemin kararı bence yine doğruydu. Tek tartışılacak pozisyon ilk yarıda bir yan topta Ujfalusi'nin Eneramo'ya sarılması ve ceza alanı içinde yere inmeleriydi. Bence penaltıydı.\")\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Tokenization and Label Encoding\n",
    "\n",
    "In this block, we use the **BERT tokenizer** (Turkish cased) to tokenize all texts. \n",
    "We also use `LabelEncoder` to convert author names into numeric IDs (0 through 29).\n",
    "By setting `max_length=256`, we truncate/pad each article to 256 tokens.\n",
    "Finally, we extract `input_ids_all` and `attention_mask_all` from the tokenizer output.\n"
   ],
   "id": "2719d6c716ca1e00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T12:31:36.559923Z",
     "start_time": "2025-01-29T12:31:27.057395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3 - Tokenization and Label Encoding\n",
    "\n",
    "# Instantiate the Turkish cased BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "\n",
    "# Separate authors and texts into two lists\n",
    "authors = [author for author, _ in preprocessed_articles]\n",
    "texts = [content for _, content in preprocessed_articles]\n",
    "\n",
    "# Convert author names to numeric IDs\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(authors)\n",
    "\n",
    "print(f\"Number of classes (authors): {len(label_encoder.classes_)}\")  # Expect 30\n",
    "\n",
    "# Tokenize all texts with BERT\n",
    "encoded_inputs = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256,  # My system is not enough for 512\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Extract input_ids and attention masks\n",
    "input_ids_all = encoded_inputs[\"input_ids\"]\n",
    "attention_mask_all = encoded_inputs[\"attention_mask\"]\n",
    "\n",
    "print(\"Tokenization completed.\")\n",
    "print(\"Shape of input_ids_all:\", input_ids_all.shape)\n"
   ],
   "id": "cdf9043d65eebea4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (authors): 30\n",
      "Tokenization completed.\n",
      "Shape of input_ids_all: torch.Size([1500, 256])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4: 5-Fold Cross Validation, Model Training, and Evaluation\n",
    "\n",
    "We use `StratifiedKFold` to split the data into 5 folds, preserving class balance. \n",
    "For each fold, we instantiate a fresh `BertForSequenceClassification` model, train it for several epochs, \n",
    "and then evaluate on the validation portion. \n",
    "We gather predictions and true labels to generate a fold-wise classification report.\n"
   ],
   "id": "a2d8c0c7bc7ade49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:22:50.810180Z",
     "start_time": "2025-01-29T12:31:36.573920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4 - 5-Fold Cross Validation, Model Training, and Evaluation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# StratifiedKFold object for 5-fold CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the true labels and predictions across all folds\n",
    "all_folds_true = []\n",
    "all_folds_pred = []\n",
    "\n",
    "fold_index = 1\n",
    "\n",
    "# Perform the 5-fold split\n",
    "for train_index, val_index in skf.split(input_ids_all, labels):\n",
    "    print(f\"\\n===== Fold {fold_index} =====\")\n",
    "    \n",
    "    # Split data into train/validation sets for this fold\n",
    "    X_train_ids = input_ids_all[train_index]\n",
    "    X_train_mask = attention_mask_all[train_index]\n",
    "    y_train = labels[train_index]\n",
    "\n",
    "    X_val_ids = input_ids_all[val_index]\n",
    "    X_val_mask = attention_mask_all[val_index]\n",
    "    y_val = labels[val_index]\n",
    "    \n",
    "    # Create TensorDatasets for train and validation\n",
    "    train_dataset = TensorDataset(X_train_ids, X_train_mask, torch.tensor(y_train))\n",
    "    val_dataset = TensorDataset(X_val_ids, X_val_mask, torch.tensor(y_val))\n",
    "    \n",
    "    # DataLoaders for batch processing\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Load the pre-trained BERT model for sequence classification\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"dbmdz/bert-base-turkish-cased\",\n",
    "        num_labels=num_labels\n",
    "    ).to(device)\n",
    "    \n",
    "    # Use AdamW optimizer from Transformers\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # CrossEntropy loss for multi-class classification\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    epochs = 4  # Number of epochs\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        correct_train = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids_batch, attn_mask_batch, labels_batch = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids_batch, attention_mask=attn_mask_batch)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, labels_batch)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Predictions for accuracy\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_train += torch.sum(preds == labels_batch)\n",
    "            \n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_acc = correct_train.double() / len(train_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids_batch, attn_mask_batch, labels_batch = [b.to(device) for b in batch]\n",
    "                outputs = model(input_ids_batch, attention_mask=attn_mask_batch)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                loss = loss_fn(logits, labels_batch)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct_val += torch.sum(preds == labels_batch)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct_val.double() / len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # After training, collect predictions on the validation set\n",
    "    model.eval()\n",
    "    fold_preds = []\n",
    "    fold_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids_batch, attn_mask_batch, labels_batch = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids_batch, attention_mask=attn_mask_batch)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            fold_preds.extend(preds.cpu().numpy())\n",
    "            fold_true.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Add results from this fold to the global lists\n",
    "    all_folds_true.extend(fold_true)\n",
    "    all_folds_pred.extend(fold_preds)\n",
    "\n",
    "    # Classification report for this fold\n",
    "    print(\"\\nClassification report for this fold:\")\n",
    "    fold_report = classification_report(\n",
    "        fold_true, \n",
    "        fold_preds, \n",
    "        target_names=label_encoder.classes_, \n",
    "        zero_division=0\n",
    "    )\n",
    "    print(fold_report)\n",
    "    \n",
    "    fold_index += 1\n"
   ],
   "id": "643bab1345f29c53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Deder\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3.3525, Train Acc: 0.0742\n",
      "Validation Loss: 2.9317, Validation Acc: 0.2133\n",
      "Epoch 2: Train Loss: 2.4873, Train Acc: 0.4117\n",
      "Validation Loss: 1.8534, Validation Acc: 0.6300\n",
      "Epoch 3: Train Loss: 1.5101, Train Acc: 0.7625\n",
      "Validation Loss: 1.1356, Validation Acc: 0.8033\n",
      "Epoch 4: Train Loss: 0.8887, Train Acc: 0.8908\n",
      "Validation Loss: 0.8070, Validation Acc: 0.8067\n",
      "\n",
      "Classification report for this fold:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      AHMET ÇAKAR       0.82      0.90      0.86        10\n",
      "       ALİ SİRMEN       0.80      0.40      0.53        10\n",
      " ATAOL BEHRAMOĞLU       0.44      0.40      0.42        10\n",
      "    ATİLLA DORSAY       0.77      1.00      0.87        10\n",
      "      AYKAN SEVER       0.89      0.80      0.84        10\n",
      "       AZİZ ÜSTEL       1.00      1.00      1.00        10\n",
      "       CAN ATAKLI       1.00      1.00      1.00        10\n",
      "      DENİZ GÖKÇE       1.00      1.00      1.00        10\n",
      "      EMRE KONGAR       0.38      0.50      0.43        10\n",
      "  GÖZDE BEDELOĞLU       0.80      0.40      0.53        10\n",
      "      HASAN PULUR       0.83      1.00      0.91        10\n",
      " HİKMET ÇETİNKAYA       0.91      1.00      0.95        10\n",
      "MEHMET ALİ BİRAND       1.00      1.00      1.00        10\n",
      "  MEHMET DEMİRKOL       1.00      1.00      1.00        10\n",
      "     MELTEM GÜRLE       0.53      0.80      0.64        10\n",
      "     MERYEM KORAY       0.80      0.80      0.80        10\n",
      "    MÜMTAZ SOYSAL       0.91      1.00      0.95        10\n",
      "  NAZAN BEKİROĞLU       0.80      0.40      0.53        10\n",
      "     NAZIM ALPMAN       0.69      0.90      0.78        10\n",
      "      NEDİM HAZAR       0.86      0.60      0.71        10\n",
      "       NEŞE YAŞIN       0.60      0.60      0.60        10\n",
      "     OKAY KARACAN       1.00      0.80      0.89        10\n",
      "      REHA MUHTAR       1.00      1.00      1.00        10\n",
      "    RIDVAN DİLMEN       0.91      1.00      0.95        10\n",
      "      RUHAT MENGİ       1.00      0.90      0.95        10\n",
      "      SELİM İLERİ       0.71      1.00      0.83        10\n",
      "     TARHAN ERDEM       0.83      1.00      0.91        10\n",
      "      UFUK BOZKIR       0.60      0.60      0.60        10\n",
      "     YAŞAR SEYMAN       1.00      0.90      0.95        10\n",
      "ÖZGE BAŞAK TANELİ       0.62      0.50      0.56        10\n",
      "\n",
      "         accuracy                           0.81       300\n",
      "        macro avg       0.82      0.81      0.80       300\n",
      "     weighted avg       0.82      0.81      0.80       300\n",
      "\n",
      "\n",
      "===== Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Deder\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3.3959, Train Acc: 0.0558\n",
      "Validation Loss: 3.1650, Validation Acc: 0.1333\n",
      "Epoch 2: Train Loss: 2.8402, Train Acc: 0.2492\n",
      "Validation Loss: 2.4392, Validation Acc: 0.4100\n",
      "Epoch 3: Train Loss: 2.0762, Train Acc: 0.5517\n",
      "Validation Loss: 1.6927, Validation Acc: 0.6633\n",
      "Epoch 4: Train Loss: 1.3346, Train Acc: 0.7683\n",
      "Validation Loss: 1.0612, Validation Acc: 0.8000\n",
      "\n",
      "Classification report for this fold:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      AHMET ÇAKAR       0.80      0.80      0.80        10\n",
      "       ALİ SİRMEN       0.86      0.60      0.71        10\n",
      " ATAOL BEHRAMOĞLU       1.00      0.50      0.67        10\n",
      "    ATİLLA DORSAY       0.64      0.90      0.75        10\n",
      "      AYKAN SEVER       1.00      0.90      0.95        10\n",
      "       AZİZ ÜSTEL       0.91      1.00      0.95        10\n",
      "       CAN ATAKLI       1.00      1.00      1.00        10\n",
      "      DENİZ GÖKÇE       1.00      1.00      1.00        10\n",
      "      EMRE KONGAR       0.53      0.90      0.67        10\n",
      "  GÖZDE BEDELOĞLU       0.50      0.10      0.17        10\n",
      "      HASAN PULUR       0.77      1.00      0.87        10\n",
      " HİKMET ÇETİNKAYA       1.00      1.00      1.00        10\n",
      "MEHMET ALİ BİRAND       1.00      1.00      1.00        10\n",
      "  MEHMET DEMİRKOL       1.00      0.90      0.95        10\n",
      "     MELTEM GÜRLE       1.00      0.80      0.89        10\n",
      "     MERYEM KORAY       0.82      0.90      0.86        10\n",
      "    MÜMTAZ SOYSAL       1.00      1.00      1.00        10\n",
      "  NAZAN BEKİROĞLU       0.67      0.60      0.63        10\n",
      "     NAZIM ALPMAN       1.00      0.20      0.33        10\n",
      "      NEDİM HAZAR       0.75      0.60      0.67        10\n",
      "       NEŞE YAŞIN       0.60      0.90      0.72        10\n",
      "     OKAY KARACAN       1.00      0.80      0.89        10\n",
      "      REHA MUHTAR       1.00      1.00      1.00        10\n",
      "    RIDVAN DİLMEN       0.83      1.00      0.91        10\n",
      "      RUHAT MENGİ       1.00      0.90      0.95        10\n",
      "      SELİM İLERİ       0.64      0.90      0.75        10\n",
      "     TARHAN ERDEM       0.77      1.00      0.87        10\n",
      "      UFUK BOZKIR       0.44      0.40      0.42        10\n",
      "     YAŞAR SEYMAN       0.50      1.00      0.67        10\n",
      "ÖZGE BAŞAK TANELİ       0.80      0.40      0.53        10\n",
      "\n",
      "         accuracy                           0.80       300\n",
      "        macro avg       0.83      0.80      0.79       300\n",
      "     weighted avg       0.83      0.80      0.79       300\n",
      "\n",
      "\n",
      "===== Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Deder\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3.3399, Train Acc: 0.0783\n",
      "Validation Loss: 2.9569, Validation Acc: 0.2767\n",
      "Epoch 2: Train Loss: 2.4431, Train Acc: 0.4442\n",
      "Validation Loss: 1.8370, Validation Acc: 0.6100\n",
      "Epoch 3: Train Loss: 1.5061, Train Acc: 0.7025\n",
      "Validation Loss: 1.2012, Validation Acc: 0.7833\n",
      "Epoch 4: Train Loss: 0.9085, Train Acc: 0.8808\n",
      "Validation Loss: 0.8442, Validation Acc: 0.8300\n",
      "\n",
      "Classification report for this fold:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      AHMET ÇAKAR       0.67      1.00      0.80        10\n",
      "       ALİ SİRMEN       0.80      0.80      0.80        10\n",
      " ATAOL BEHRAMOĞLU       1.00      0.80      0.89        10\n",
      "    ATİLLA DORSAY       0.82      0.90      0.86        10\n",
      "      AYKAN SEVER       1.00      0.70      0.82        10\n",
      "       AZİZ ÜSTEL       1.00      1.00      1.00        10\n",
      "       CAN ATAKLI       0.91      1.00      0.95        10\n",
      "      DENİZ GÖKÇE       1.00      1.00      1.00        10\n",
      "      EMRE KONGAR       0.70      0.70      0.70        10\n",
      "  GÖZDE BEDELOĞLU       0.80      0.40      0.53        10\n",
      "      HASAN PULUR       0.83      1.00      0.91        10\n",
      " HİKMET ÇETİNKAYA       0.91      1.00      0.95        10\n",
      "MEHMET ALİ BİRAND       0.91      1.00      0.95        10\n",
      "  MEHMET DEMİRKOL       1.00      1.00      1.00        10\n",
      "     MELTEM GÜRLE       0.62      0.80      0.70        10\n",
      "     MERYEM KORAY       0.64      0.90      0.75        10\n",
      "    MÜMTAZ SOYSAL       0.90      0.90      0.90        10\n",
      "  NAZAN BEKİROĞLU       0.62      0.50      0.56        10\n",
      "     NAZIM ALPMAN       0.88      0.70      0.78        10\n",
      "      NEDİM HAZAR       0.86      0.60      0.71        10\n",
      "       NEŞE YAŞIN       0.82      0.90      0.86        10\n",
      "     OKAY KARACAN       1.00      0.80      0.89        10\n",
      "      REHA MUHTAR       1.00      1.00      1.00        10\n",
      "    RIDVAN DİLMEN       1.00      0.70      0.82        10\n",
      "      RUHAT MENGİ       0.77      1.00      0.87        10\n",
      "      SELİM İLERİ       0.77      1.00      0.87        10\n",
      "     TARHAN ERDEM       1.00      0.70      0.82        10\n",
      "      UFUK BOZKIR       0.57      0.40      0.47        10\n",
      "     YAŞAR SEYMAN       0.75      0.90      0.82        10\n",
      "ÖZGE BAŞAK TANELİ       0.73      0.80      0.76        10\n",
      "\n",
      "         accuracy                           0.83       300\n",
      "        macro avg       0.84      0.83      0.82       300\n",
      "     weighted avg       0.84      0.83      0.82       300\n",
      "\n",
      "\n",
      "===== Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Deder\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3.3460, Train Acc: 0.0767\n",
      "Validation Loss: 2.9600, Validation Acc: 0.2033\n",
      "Epoch 2: Train Loss: 2.4911, Train Acc: 0.4042\n",
      "Validation Loss: 1.8815, Validation Acc: 0.6100\n",
      "Epoch 3: Train Loss: 1.5570, Train Acc: 0.7108\n",
      "Validation Loss: 1.2803, Validation Acc: 0.7400\n",
      "Epoch 4: Train Loss: 0.9433, Train Acc: 0.8708\n",
      "Validation Loss: 0.8327, Validation Acc: 0.8133\n",
      "\n",
      "Classification report for this fold:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      AHMET ÇAKAR       0.59      1.00      0.74        10\n",
      "       ALİ SİRMEN       1.00      0.30      0.46        10\n",
      " ATAOL BEHRAMOĞLU       0.58      0.70      0.64        10\n",
      "    ATİLLA DORSAY       0.89      0.80      0.84        10\n",
      "      AYKAN SEVER       0.80      0.80      0.80        10\n",
      "       AZİZ ÜSTEL       1.00      1.00      1.00        10\n",
      "       CAN ATAKLI       1.00      1.00      1.00        10\n",
      "      DENİZ GÖKÇE       1.00      1.00      1.00        10\n",
      "      EMRE KONGAR       0.70      0.70      0.70        10\n",
      "  GÖZDE BEDELOĞLU       0.80      0.40      0.53        10\n",
      "      HASAN PULUR       0.62      1.00      0.77        10\n",
      " HİKMET ÇETİNKAYA       0.91      1.00      0.95        10\n",
      "MEHMET ALİ BİRAND       1.00      1.00      1.00        10\n",
      "  MEHMET DEMİRKOL       1.00      1.00      1.00        10\n",
      "     MELTEM GÜRLE       0.69      0.90      0.78        10\n",
      "     MERYEM KORAY       0.89      0.80      0.84        10\n",
      "    MÜMTAZ SOYSAL       0.77      1.00      0.87        10\n",
      "  NAZAN BEKİROĞLU       0.67      1.00      0.80        10\n",
      "     NAZIM ALPMAN       0.83      1.00      0.91        10\n",
      "      NEDİM HAZAR       1.00      0.70      0.82        10\n",
      "       NEŞE YAŞIN       0.83      0.50      0.62        10\n",
      "     OKAY KARACAN       1.00      0.30      0.46        10\n",
      "      REHA MUHTAR       1.00      1.00      1.00        10\n",
      "    RIDVAN DİLMEN       1.00      0.90      0.95        10\n",
      "      RUHAT MENGİ       0.89      0.80      0.84        10\n",
      "      SELİM İLERİ       0.77      1.00      0.87        10\n",
      "     TARHAN ERDEM       1.00      0.90      0.95        10\n",
      "      UFUK BOZKIR       0.80      0.40      0.53        10\n",
      "     YAŞAR SEYMAN       0.60      0.90      0.72        10\n",
      "ÖZGE BAŞAK TANELİ       0.67      0.60      0.63        10\n",
      "\n",
      "         accuracy                           0.81       300\n",
      "        macro avg       0.84      0.81      0.80       300\n",
      "     weighted avg       0.84      0.81      0.80       300\n",
      "\n",
      "\n",
      "===== Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Deder\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3.3757, Train Acc: 0.0592\n",
      "Validation Loss: 3.0359, Validation Acc: 0.1800\n",
      "Epoch 2: Train Loss: 2.4907, Train Acc: 0.4067\n",
      "Validation Loss: 1.8428, Validation Acc: 0.5867\n",
      "Epoch 3: Train Loss: 1.5171, Train Acc: 0.7258\n",
      "Validation Loss: 1.1847, Validation Acc: 0.7800\n",
      "Epoch 4: Train Loss: 0.9182, Train Acc: 0.8800\n",
      "Validation Loss: 0.7886, Validation Acc: 0.8333\n",
      "\n",
      "Classification report for this fold:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      AHMET ÇAKAR       1.00      0.90      0.95        10\n",
      "       ALİ SİRMEN       0.36      0.90      0.51        10\n",
      " ATAOL BEHRAMOĞLU       0.00      0.00      0.00        10\n",
      "    ATİLLA DORSAY       0.83      1.00      0.91        10\n",
      "      AYKAN SEVER       0.90      0.90      0.90        10\n",
      "       AZİZ ÜSTEL       1.00      1.00      1.00        10\n",
      "       CAN ATAKLI       1.00      1.00      1.00        10\n",
      "      DENİZ GÖKÇE       1.00      1.00      1.00        10\n",
      "      EMRE KONGAR       1.00      0.20      0.33        10\n",
      "  GÖZDE BEDELOĞLU       0.80      0.80      0.80        10\n",
      "      HASAN PULUR       0.77      1.00      0.87        10\n",
      " HİKMET ÇETİNKAYA       0.90      0.90      0.90        10\n",
      "MEHMET ALİ BİRAND       1.00      1.00      1.00        10\n",
      "  MEHMET DEMİRKOL       1.00      0.90      0.95        10\n",
      "     MELTEM GÜRLE       0.62      0.80      0.70        10\n",
      "     MERYEM KORAY       0.90      0.90      0.90        10\n",
      "    MÜMTAZ SOYSAL       0.83      1.00      0.91        10\n",
      "  NAZAN BEKİROĞLU       0.70      0.70      0.70        10\n",
      "     NAZIM ALPMAN       1.00      0.70      0.82        10\n",
      "      NEDİM HAZAR       0.83      0.50      0.62        10\n",
      "       NEŞE YAŞIN       0.75      0.90      0.82        10\n",
      "     OKAY KARACAN       0.91      1.00      0.95        10\n",
      "      REHA MUHTAR       0.83      1.00      0.91        10\n",
      "    RIDVAN DİLMEN       1.00      1.00      1.00        10\n",
      "      RUHAT MENGİ       1.00      1.00      1.00        10\n",
      "      SELİM İLERİ       1.00      0.70      0.82        10\n",
      "     TARHAN ERDEM       1.00      1.00      1.00        10\n",
      "      UFUK BOZKIR       0.54      0.70      0.61        10\n",
      "     YAŞAR SEYMAN       1.00      1.00      1.00        10\n",
      "ÖZGE BAŞAK TANELİ       0.86      0.60      0.71        10\n",
      "\n",
      "         accuracy                           0.83       300\n",
      "        macro avg       0.84      0.83      0.82       300\n",
      "     weighted avg       0.84      0.83      0.82       300\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 5: Combining All Folds and Final Report\n",
    "\n",
    "Finally, we combine the predictions from all 5 folds and generate an overall \n",
    "classification report on all 1500 articles (i.e., the entire dataset). \n",
    "This fulfills the requirement for a complete 5-fold CV result. \n",
    "We also save the final report to a `.txt` file.\n"
   ],
   "id": "c342f3a32822ca76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:22:51.365418Z",
     "start_time": "2025-01-29T13:22:51.338418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 5 - Combining All Folds and Final Report\n",
    "\n",
    "print(\"\\n=== 5-Fold Cross-Validation Results (All Folds Combined) ===\")\n",
    "final_report = classification_report(\n",
    "    all_folds_true,\n",
    "    all_folds_pred,\n",
    "    target_names=label_encoder.classes_,\n",
    "    zero_division=0\n",
    ")\n",
    "print(final_report)\n",
    "\n",
    "# Save the final report to a text file\n",
    "with open(\"bert_5fold_cv_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(\"5-fold CV report has been saved to 'bert_5fold_cv_report.txt'.\")\n"
   ],
   "id": "6b2c08ee6599a9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 5-Fold Cross-Validation Results (All Folds Combined) ===\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      AHMET ÇAKAR       0.74      0.92      0.82        50\n",
      "       ALİ SİRMEN       0.60      0.60      0.60        50\n",
      " ATAOL BEHRAMOĞLU       0.71      0.48      0.57        50\n",
      "    ATİLLA DORSAY       0.78      0.92      0.84        50\n",
      "      AYKAN SEVER       0.91      0.82      0.86        50\n",
      "       AZİZ ÜSTEL       0.98      1.00      0.99        50\n",
      "       CAN ATAKLI       0.98      1.00      0.99        50\n",
      "      DENİZ GÖKÇE       1.00      1.00      1.00        50\n",
      "      EMRE KONGAR       0.58      0.60      0.59        50\n",
      "  GÖZDE BEDELOĞLU       0.78      0.42      0.55        50\n",
      "      HASAN PULUR       0.76      1.00      0.86        50\n",
      " HİKMET ÇETİNKAYA       0.92      0.98      0.95        50\n",
      "MEHMET ALİ BİRAND       0.98      1.00      0.99        50\n",
      "  MEHMET DEMİRKOL       1.00      0.96      0.98        50\n",
      "     MELTEM GÜRLE       0.66      0.82      0.73        50\n",
      "     MERYEM KORAY       0.80      0.86      0.83        50\n",
      "    MÜMTAZ SOYSAL       0.88      0.98      0.92        50\n",
      "  NAZAN BEKİROĞLU       0.68      0.64      0.66        50\n",
      "     NAZIM ALPMAN       0.83      0.70      0.76        50\n",
      "      NEDİM HAZAR       0.86      0.60      0.71        50\n",
      "       NEŞE YAŞIN       0.70      0.76      0.73        50\n",
      "     OKAY KARACAN       0.97      0.74      0.84        50\n",
      "      REHA MUHTAR       0.96      1.00      0.98        50\n",
      "    RIDVAN DİLMEN       0.94      0.92      0.93        50\n",
      "      RUHAT MENGİ       0.92      0.92      0.92        50\n",
      "      SELİM İLERİ       0.75      0.92      0.83        50\n",
      "     TARHAN ERDEM       0.90      0.92      0.91        50\n",
      "      UFUK BOZKIR       0.57      0.50      0.53        50\n",
      "     YAŞAR SEYMAN       0.71      0.94      0.81        50\n",
      "ÖZGE BAŞAK TANELİ       0.72      0.58      0.64        50\n",
      "\n",
      "         accuracy                           0.82      1500\n",
      "        macro avg       0.82      0.82      0.81      1500\n",
      "     weighted avg       0.82      0.82      0.81      1500\n",
      "\n",
      "5-fold CV report has been saved to 'bert_5fold_cv_report.txt'.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:30:02.816699Z",
     "start_time": "2025-01-29T15:30:02.808166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- ADD THIS AT THE VERY END, AFTER ALL_FOLDS_TRUE and ALL_FOLDS_PREDS ARE COLLECTED ---\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# We assume 'num_labels' == 30 and 'label_encoder.classes_' maps 0..29 to your 30 authors.\n",
    "# First, compute per-class precision, recall, and f1\n",
    "precisions, recalls, f1_scores, _ = precision_recall_fscore_support(\n",
    "    all_folds_true, \n",
    "    all_folds_pred, \n",
    "    labels=range(num_labels),  # ensure we cover all classes in correct order\n",
    "    zero_division=0            # avoids division-by-zero warnings\n",
    ")\n",
    "\n",
    "# Calculate macro-average (or use micro if you prefer)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "# Print the table header\n",
    "print(\"\\nPerformance Results\")\n",
    "header_row = \"\\t\".join([f\"Class {i+1}\" for i in range(num_labels)]) + \"\\tAverage\"\n",
    "print(\" \" * 11 + header_row)\n",
    "\n",
    "# Print precision row\n",
    "precision_row = \"\\t\".join(f\"{p:.2f}\" for p in precisions) + f\"\\t{avg_precision:.2f}\"\n",
    "print(f\"Precision  {precision_row}\")\n",
    "\n",
    "# Print recall row\n",
    "recall_row = \"\\t\".join(f\"{r:.2f}\" for r in recalls) + f\"\\t{avg_recall:.2f}\"\n",
    "print(f\"Recall     {recall_row}\")\n",
    "\n",
    "# Print F-score row\n",
    "fscore_row = \"\\t\".join(f\"{f:.2f}\" for f in f1_scores) + f\"\\t{avg_f1_score:.2f}\"\n",
    "print(f\"F-Score    {fscore_row}\")\n"
   ],
   "id": "fa045f2da50c45f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Results\n",
      "           Class 1\tClass 2\tClass 3\tClass 4\tClass 5\tClass 6\tClass 7\tClass 8\tClass 9\tClass 10\tClass 11\tClass 12\tClass 13\tClass 14\tClass 15\tClass 16\tClass 17\tClass 18\tClass 19\tClass 20\tClass 21\tClass 22\tClass 23\tClass 24\tClass 25\tClass 26\tClass 27\tClass 28\tClass 29\tClass 30\tAverage\n",
      "Precision  0.74\t0.60\t0.71\t0.78\t0.91\t0.98\t0.98\t1.00\t0.58\t0.78\t0.76\t0.92\t0.98\t1.00\t0.66\t0.80\t0.88\t0.68\t0.83\t0.86\t0.70\t0.97\t0.96\t0.94\t0.92\t0.75\t0.90\t0.57\t0.71\t0.72\t0.82\n",
      "Recall     0.92\t0.60\t0.48\t0.92\t0.82\t1.00\t1.00\t1.00\t0.60\t0.42\t1.00\t0.98\t1.00\t0.96\t0.82\t0.86\t0.98\t0.64\t0.70\t0.60\t0.76\t0.74\t1.00\t0.92\t0.92\t0.92\t0.92\t0.50\t0.94\t0.58\t0.82\n",
      "F-Score    0.82\t0.60\t0.57\t0.84\t0.86\t0.99\t0.99\t1.00\t0.59\t0.55\t0.86\t0.95\t0.99\t0.98\t0.73\t0.83\t0.92\t0.66\t0.76\t0.71\t0.73\t0.84\t0.98\t0.93\t0.92\t0.83\t0.91\t0.53\t0.81\t0.64\t0.81\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
